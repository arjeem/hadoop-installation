<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="UTF-8">
    <title>Single Node Hadoop Installation on Ubuntu 14.04 Amazon EC2 Instance by ashishtam</title>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="stylesheets/normalize.css" media="screen">
    <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="stylesheets/github-light.css" media="screen">
  </head>
  <body>
    <section class="page-header">
      <h1 class="project-name">Single Node Hadoop Installation on Ubuntu 14.04 Amazon EC2 Instance</h1>
      <h2 class="project-tagline">by Ashish Tamrakar (UNLV)</h2>
      <a href="https://github.com/ashishtam/hadoop-installation" class="btn">View on GitHub</a>
      <a href="https://github.com/ashishtam/hadoop-installation/zipball/master" class="btn">Download .zip</a>
      <a href="https://github.com/ashishtam/hadoop-installation/tarball/master" class="btn">Download .tar.gz</a>
    </section>

    <section class="main-content">
      <h1>
<a id="create-the-amazon-ec2-instance-ubuntu-1404" class="anchor" href="#create-the-amazon-ec2-instance-ubuntu-1404" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Create the Amazon EC2 Instance (Ubuntu 14.04)</h1>

<p>Steps to set up the Amazon AWS as,</p>

<h3>
<a id="1-sign-in-to-the-aws-console-amazon-web-services" class="anchor" href="#1-sign-in-to-the-aws-console-amazon-web-services" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>1. Sign in to the AWS Console (Amazon Web Services)</h3>

<p><img src="images/1-Amazon-Web-Services.PNG" alt="Amazon Web Services"></p>

<h3>
<a id="2-launch-the-amazon-ec2-instance" class="anchor" href="#2-launch-the-amazon-ec2-instance" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>2. Launch the Amazon EC2 Instance</h3>

<p><img src="images/2-launch-instance.PNG" alt="Launch Instance"></p>

<h3>
<a id="3-selecting-the-amazon-machine-image" class="anchor" href="#3-selecting-the-amazon-machine-image" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>3. Selecting the Amazon Machine Image</h3>

<p><img src="images/3-Amazon-Machine-Image.PNG" alt="Amazon Machine Image"></p>

<h3>
<a id="4-choose-an-instance-type" class="anchor" href="#4-choose-an-instance-type" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>4. Choose an Instance Type</h3>

<p><img src="images/4-Instance-Type.PNG" alt="Instance Type"></p>

<h3>
<a id="5-configure-instance-details" class="anchor" href="#5-configure-instance-details" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>5. Configure Instance Details</h3>

<p><img src="images/5-Configuring-Instance-details.PNG" alt="Configuring Instance Details"></p>

<h3>
<a id="6-configure-security-group" class="anchor" href="#6-configure-security-group" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>6. Configure Security Group</h3>

<p><img src="images/6-Configure-Security-Group.PNG" alt="Configuring Security Group"></p>

<h3>
<a id="7-review-and-launch-instance" class="anchor" href="#7-review-and-launch-instance" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>7. Review and Launch Instance</h3>

<p><img src="images/7-Review-and-launch.PNG" alt="Review and Launch Instance"></p>

<h3>
<a id="8-create-key-pair" class="anchor" href="#8-create-key-pair" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>8. Create key pair</h3>

<p><img src="images/8-Create-key-pair.PNG" alt="Create Key Pair"></p>

<h3>
<a id="9-running-instances" class="anchor" href="#9-running-instances" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>9. Running instances</h3>

<p>Now the instance is running and you click on the Instances on the left tab to see your EC2 instances running.
<img src="images/9-Instances.png" alt="Running Instances"></p>

<h2>
<a id="to-connect-your-amazon-ec2-instance-using-ssh-from-your-local-machine-ubuntu" class="anchor" href="#to-connect-your-amazon-ec2-instance-using-ssh-from-your-local-machine-ubuntu" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>To connect your Amazon EC2 instance using SSH from your local machine (Ubuntu)</h2>

<p>In a command line shell, change directories to the location of the private key file that you created when you launched the instance.</p>

<p>Use the <code>chmod</code> command to make sure your private key file isn't publicly viewable. For example, if the name of your private key file is <code>big-data.pem</code>, use the following command:</p>

<pre><code>$ chmod 400 big-data.pem
</code></pre>

<p>Use the <code>ssh</code> command to connect to the instance. You'll specify the private key <code>(.pem)</code> file and <code>user_name@public_dns_name</code>. For Ubuntu, the username is <code>ubuntu</code>. </p>

<pre><code>ssh -i big-data.pem ubuntu@ec2-198-51-100-1.compute-1.amazonaws.com
</code></pre>

<p>You'll see a response like the following:</p>

<pre><code>The authenticity of host 'ec2-198-51-100-1.compute-1.amazonaws.com (10.254.142.33)' can't be established.
RSA key fingerprint is 1f:51:ae:28:bf:89:e9:d8:1f:25:5d:37:2d:7d:b8:ca:9f:f5:f1:6f.
Are you sure you want to continue connecting (yes/no)?
</code></pre>

<p>(Optional) Verify that the fingerprint in the security alert matches the fingerprint that you obtained in step 1. If these fingerprints don't match, someone might be attempting a "man-in-the-middle" attack. If they match, continue to the next step.
Enter <code>yes</code>.
You'll see a response like the following.</p>

<pre><code>Warning: Permanently added 'ec2-198-51-100-1.compute-1.amazonaws.com' (RSA) 
to the list of known hosts.
</code></pre>

<p>It looks something like this,</p>

<p><img src="images/10-ssh-to-ec2-instance-from-local-machine.png" alt="SSH to EC2 Instance from local machine"></p>

<p>Reference: <a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html">http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html</a></p>

<h2>
<a id="installation-of-single-node-hadoop-cluster-on-ubuntu-1404-amazon-ec2-instance" class="anchor" href="#installation-of-single-node-hadoop-cluster-on-ubuntu-1404-amazon-ec2-instance" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Installation of Single Node Hadoop cluster on Ubuntu 14.04 Amazon EC2 Instance</h2>

<h3>
<a id="install-java" class="anchor" href="#install-java" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install Java</h3>

<p>You need to install java by following commands as,</p>

<pre><code>$ sudo add-apt-repository ppa:webupd8team/java
$ sudo apt-get update
$ sudo apt-get install oracle-java7-installer
# Update Java runtime
$ sudo update-java-alternatives -s java-7-oracle
</code></pre>

<p>To check whether your java is installed or not,</p>

<pre><code>$ java -version
</code></pre>

<h3>
<a id="disabling-ipv6" class="anchor" href="#disabling-ipv6" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Disabling IPv6</h3>

<p>To disable <code>ipv6</code>, you have to open <code>/etc/sysctl.conf</code> using any text editor and insert the following lines at the end:</p>

<pre><code>net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
</code></pre>

<p>If <code>ipv6</code> is still not disabled, then the problem is that <code>sysctl.conf</code> is still not activated.</p>

<p>To solve this, open a terminal(Ctrl+Alt+T) and type the command,</p>

<pre><code>$ sudo sysctl -p
</code></pre>

<p>You will see this in the terminal:</p>

<pre><code>net.ipv6.conf.all.disable_ipv6 = 1
net.ipv6.conf.default.disable_ipv6 = 1
net.ipv6.conf.lo.disable_ipv6 = 1
</code></pre>

<p>After that, if you run:</p>

<pre><code>$ cat /proc/sys/net/ipv6/conf/all/disable_ipv6
</code></pre>

<p>It will report:</p>

<pre><code>1
</code></pre>

<h3>
<a id="install-hadoop-260-stable-binaries" class="anchor" href="#install-hadoop-260-stable-binaries" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Install Hadoop 2.6.0 stable binaries</h3>

<p>Install the hadoop binary and then install it on the home folder itself.</p>

<pre><code>$ cd
$ wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz
$ tar xvf hadoop-2.6.0.tar.gz
$ mv hadoop-2.6.0 hadoop
</code></pre>

<h3>
<a id="create-and-setup-ssh-certificates" class="anchor" href="#create-and-setup-ssh-certificates" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Create and Setup SSH Certificates</h3>

<p>To create the ssh key, we create the following command as,</p>

<pre><code>$ ssh-keygen -t rsa -P ""


Generating public/private rsa key pair.
Enter file in which to save the key (/home/ubuntu/.ssh/id_rsa): 
Created directory '/home/ubuntu/.ssh'.
Your identification has been saved in /home/ubuntu/.ssh/id_rsa.
Your public key has been saved in /home/ubuntu/.ssh/id_rsa.pub.
The key fingerprint is:
50:6b:f3:fc:0f:32:bf:30:79:c2:41:71:26:cc:7d:e3 ubuntu@laptop
The key's randomart image is:
+--[ RSA 2048]----+
|        .oo.o    |
|       . .o=. o  |
|      . + .  o . |
|       o =    E  |
|        S +      |
|         . +     |
|          O +    |
|           O o   |
|            o..  |
+-----------------+
</code></pre>

<p>You need to copy the key to the <code>authorized_keys</code>,</p>

<pre><code>$ cat $HOME/.ssh/id_rsa.pub &gt;&gt; $HOME/.ssh/authorized_keys
</code></pre>

<h2>
<a id="setting-up-hadoop-environment" class="anchor" href="#setting-up-hadoop-environment" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Setting Up Hadoop Environment</h2>

<h3>
<a id="configuring-bashrc-file" class="anchor" href="#configuring-bashrc-file" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Configuring .bashrc file</h3>

<p>You need to add the following lines on <code>.bashrc</code> file under the folder <code>/home/ubuntu</code> as,</p>

<pre><code>$ vim /home/ubuntu/.bashrc
</code></pre>

<p>Add the following lines:</p>

<pre><code>#HADOOP VARIABLES START
export JAVA_HOME=/usr/lib/jvm/java-7-oracle
export HADOOP_INSTALL=/home/ubuntu/hadoop
export PATH=$PATH:$HADOOP_INSTALL/bin
export PATH=$PATH:$HADOOP_INSTALL/sbin
export HADOOP_MAPRED_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_HOME=$HADOOP_INSTALL
export HADOOP_HDFS_HOME=$HADOOP_INSTALL
export YARN_HOME=$HADOOP_INSTALL
export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native
export HADOOP_OPTS="-Djava.library.path=$HADOOP_INSTALL/lib/native"
export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar
#HADOOP VARIABLES END
</code></pre>

<p>You need to re-load your <code>.bashrc</code> configuration. In order to reload, you can simply execute the following command as,</p>

<pre><code>$ source ~/.bashrc
</code></pre>

<h3>
<a id="update-hadoop-envsh" class="anchor" href="#update-hadoop-envsh" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Update hadoop-env.sh</h3>

<p>Update the <code>JAVA_HOME</code> in <code>/home/ubuntu/hadoop/etc/hadoop/hadoop-env.sh</code> as,</p>

<pre><code>$ vim /home/ubuntu/hadoop/etc/hadoop/hadoop-env.sh
</code></pre>

<p>Find the <code>export JAVA_HOME</code> line and replace <code>{JAVA_HOME}</code> with the actual path as,</p>

<pre><code>#Find this line and update with the path
export JAVA_HOME=/usr/lib/jvm/java-7-oracle
</code></pre>

<h2>
<a id="configuring-hadoop-configuration-files" class="anchor" href="#configuring-hadoop-configuration-files" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Configuring hadoop configuration files</h2>

<h3>
<a id="addupdate-core-sitexml" class="anchor" href="#addupdate-core-sitexml" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Add/Update core-site.xml</h3>

<p>Add/Update the <code>core-site.xml</code> file by running the following command as,</p>

<pre><code>$ vim /home/ubuntu/hadoop/etc/hadoop/core-site.xml
</code></pre>

<p>Add the following code under the <code>&lt;configuration&gt;&lt;/configuration&gt;</code> as,</p>

<pre><code>&lt;configuration&gt;
&lt;property&gt;
  &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
  &lt;value&gt;/home/ubuntu/hadoop/tmp&lt;/value&gt;
  &lt;description&gt;Temporary Directory.&lt;/description&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;fs.default.name&lt;/name&gt;
  &lt;value&gt;hdfs://localhost:54310&lt;/value&gt;
  &lt;description&gt;Use HDFS as file storage engine&lt;/description&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>Also, you need to create the <code>tmp</code> folder on <code>/home/ubuntu/hadoop</code> as,</p>

<pre><code>$ mkdir /home/ubuntu/hadoop/tmp
</code></pre>

<h3>
<a id="addupdate-mapred-sitexml" class="anchor" href="#addupdate-mapred-sitexml" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Add/update mapred-site.xml</h3>

<p>If you find the <code>mapred-site.xml</code> file inside <code>/home/ubuntu/hadoop/etc/hadoop/</code> then you just need to update inside the <code>mapred-site.xml</code> file as shown below. 
Otherwise, you need to copy the <code>mapred-site.xml.template</code> to create <code>mapred-site.xml</code> file as,</p>

<p>Commands to create the file (Skip this process if there is already <code>mapred-site.xml</code> file inside <code>/home/ubuntu/hadoop/etc/hadoop</code> folder),</p>

<pre><code>$ cd /home/ubuntu/hadoop/etc/hadoop
$ cp mapred-site.xml.template mapred-site.xml
$ vim mapred-site.xml
</code></pre>

<p>Now, update <code>mapred-site.xml</code> file under the <code>&lt;configuration&gt;&lt;/configuration&gt;</code> as,</p>

<pre><code>&lt;configuration&gt;
    &lt;property&gt;
    &lt;name&gt;mapred.job.tracker&lt;/name&gt;
    &lt;value&gt;localhost:54311&lt;/value&gt;
    &lt;description&gt;The host and port that the MapReduce job tracker runs
    at.  If "local", then jobs are run in-process as a single map
    and reduce task.
    &lt;/description&gt;
 &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<h3>
<a id="addupdate-hdfs-sitexml" class="anchor" href="#addupdate-hdfs-sitexml" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Add/update hdfs-site.xml</h3>

<p>Updating the <code>hdfs-site.xml</code> file as,</p>

<pre><code>$ vim /home/ubuntu/hadoop/etc/hadoop/hdfs-site.xml
</code></pre>

<p>Edit the <code>hdfs-site.xml</code> file under the <code>&lt;configuration&gt;&lt;/configuration&gt;</code> as,</p>

<pre><code>&lt;configuration&gt;
  &lt;property&gt;
    &lt;name&gt;dfs.replication&lt;/name&gt;
    &lt;value&gt;1&lt;/value&gt;
    &lt;description&gt;Default block replication.
    The actual number of replications can be specified when the file is created.
    The default is used if replication is not specified in create time.
    &lt;/description&gt;
  &lt;/property&gt;
  &lt;property&gt;
     &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;
     &lt;value&gt;file:///home/ubuntu/hadoop/hdfs/namenode&lt;/value&gt;
  &lt;/property&gt;
  &lt;property&gt;
     &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;
     &lt;value&gt;file:///home/ubuntu/hadoop/hdfs/datanode&lt;/value&gt;
  &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<p>Now you need to create two folders: <code>namenode</code> and <code>datanode</code> under the folders <code>/home/ubuntu/hadoop/hdfs</code> as,</p>

<pre><code>$ mkdir -p /home/ubuntu/hadoop/hdfs/namenode
$ mkdir -p /home/ubuntu/hadoop/hdfs/datanode
</code></pre>

<h3>
<a id="addupdate-yarn-sitexml" class="anchor" href="#addupdate-yarn-sitexml" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Add/update yarn-site.xml</h3>

<p>Updating the <code>yarn-site.xml</code> file as,</p>

<pre><code>$ vim /home/ubuntu/hadoop/etc/hadoop/yarn-site.xml
</code></pre>

<p>Edit the <code>yarn-site.xml</code> file under the <code>&lt;configuration&gt;&lt;/configuration&gt;</code> as,</p>

<pre><code>&lt;configuration&gt;

&lt;!-- Site specific YARN configuration properties --&gt;
&lt;property&gt;
 &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
 &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
 &lt;name&gt;yarn.resourcemanager.scheduler.address&lt;/name&gt;
 &lt;value&gt;localhost:8030&lt;/value&gt;
&lt;/property&gt; 
&lt;property&gt;
 &lt;name&gt;yarn.resourcemanager.address&lt;/name&gt;
 &lt;value&gt;localhost:8032&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;yarn.resourcemanager.webapp.address&lt;/name&gt;
  &lt;value&gt;localhost:8088&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;yarn.resourcemanager.resource-tracker.address&lt;/name&gt;
  &lt;value&gt;localhost:8031&lt;/value&gt;
&lt;/property&gt;
&lt;property&gt;
  &lt;name&gt;yarn.resourcemanager.admin.address&lt;/name&gt;
  &lt;value&gt;localhost:8033&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;
</code></pre>

<h2>
<a id="format-the-new-hadoop-file-system" class="anchor" href="#format-the-new-hadoop-file-system" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Format the New Hadoop File System</h2>

<p>Now, the Hadoop file system needs to be formatted so that we can start to use it. The format command should be issued with write permission since it creates current directory</p>

<pre><code>$ hadoop namenode -format
</code></pre>

<h2>
<a id="starting-hadoop" class="anchor" href="#starting-hadoop" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Starting Hadoop</h2>

<p>To start the hadoop, we use <code>start-all.sh</code> or (<code>start-dfs.sh</code> and <code>start-yarn.sh</code> individually) as</p>

<pre><code>$ start-all.sh
</code></pre>

<p>OR</p>

<pre><code>$ start-dfs.sh
$ start-yarn.sh
</code></pre>

<p>The command will result in output as,</p>

<pre><code>ubuntu@ip-172-31-50-38:~$ start-all.sh
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
Starting namenodes on [localhost]
localhost: starting namenode, logging to /home/ubuntu/hadoop/logs/hadoop-ubuntu-namenode-ip-172-31-50-38.out
localhost: starting datanode, logging to /home/ubuntu/hadoop/logs/hadoop-ubuntu-datanode-ip-172-31-50-38.out
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /home/ubuntu/hadoop/logs/hadoop-ubuntu-secondarynamenode-ip-172-31-50-38.out
starting yarn daemons
starting resourcemanager, logging to /home/ubuntu/hadoop/logs/yarn-ubuntu-resourcemanager-ip-172-31-50-38.out
localhost: starting nodemanager, logging to /home/ubuntu/hadoop/logs/yarn-ubuntu-nodemanager-ip-172-31-50-38.out
</code></pre>

<p>We can check if it's really up and running:</p>

<pre><code>$ jps
5180 SecondaryNameNode
3567 ResourceManager
5412 NodeManager
5442 Jps
4836 NameNode
4993 DataNode
</code></pre>

<h2>
<a id="testing-mapreduce-job" class="anchor" href="#testing-mapreduce-job" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Testing MapReduce Job</h2>

<p>For testing MapReduce job, you can test it with the jar files inside share/hadoop/mapreduce folder as,</p>

<pre><code>~/hadoop$ hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar pi 2 5
Number of Maps  = 2
Samples per Map = 5
14/07/14 01:28:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Wrote input for Map #0
Wrote input for Map #1
Starting Job
14/07/14 01:28:07 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
14/07/14 01:28:07 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
14/07/14 01:28:07 INFO input.FileInputFormat: Total input paths to process : 2
14/07/14 01:28:07 INFO mapreduce.JobSubmitter: number of splits:2
14/07/14 01:28:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1228885165_0001
...
    File Input Format Counters 
        Bytes Read=236
    File Output Format Counters 
        Bytes Written=97
Job Finished in 6.072 seconds
Estimated value of Pi is 3.60000000000000000000
</code></pre>

<h2>
<a id="mapreduce-example-with-wordcount" class="anchor" href="#mapreduce-example-with-wordcount" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>MapReduce example with WordCount</h2>

<p>We have a file <code>wordcount.txt</code> which has the following content as,</p>

<pre><code>$ cat wordcount.txt
Hello Hadoop Goodbye Hadoop
</code></pre>

<p>Let's look at Hadoop file system:</p>

<pre><code>$ hadoop fs -ls
</code></pre>

<p>Now we want to copy the input file to hdfs:</p>

<pre><code>$ hadoop fs -mkdir input
$ hadoop fs -put wordcount.txt input/
$ hadoop fs -ls input
$ hadoop fs -cat input/wordcount.txt
Hello Hadoop Goodbye Hadoop
</code></pre>

<h3>
<a id="creating-java-file-for-wordcount" class="anchor" href="#creating-java-file-for-wordcount" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Creating Java file for WordCount</h3>

<pre><code>import java.io.IOException;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {

  public static class TokenizerMapper
       extends Mapper&lt;Object, Text, Text, IntWritable&gt;{

    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();

    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken());
        context.write(word, one);
      }
    }
  }

  public static class IntSumReducer
       extends Reducer&lt;Text,IntWritable,Text,IntWritable&gt; {
    private IntWritable result = new IntWritable();

    public void reduce(Text key, Iterable&lt;IntWritable&gt; values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }

  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}
</code></pre>

<h3>
<a id="making-jar-file-from-the-java-code" class="anchor" href="#making-jar-file-from-the-java-code" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Making jar file from the java code</h3>

<pre><code>$ hadoop com.sun.tools.javac.Main WordCount.java 
$ jar cf wordcount.jar WordCount*.class
</code></pre>

<h3>
<a id="running-wordcount-mapreduce" class="anchor" href="#running-wordcount-mapreduce" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Running wordcount MapReduce</h3>

<pre><code>ubuntu@ip-172-31-50-38:~/hadoop/hadoopTest/WordCount$ hadoop jar wordcount.jar WordCount input/wordcount.txt output1
16/03/08 05:40:56 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id
16/03/08 05:40:56 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
16/03/08 05:40:57 WARN mapreduce.JobSubmitter: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.
16/03/08 05:40:57 INFO input.FileInputFormat: Total input paths to process : 1
16/03/08 05:40:57 INFO mapreduce.JobSubmitter: number of splits:1
16/03/08 05:40:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2071963236_0001
16/03/08 05:40:57 INFO mapreduce.Job: The url to track the job: http://localhost:8080/
16/03/08 05:40:57 INFO mapreduce.Job: Running job: job_local2071963236_0001
16/03/08 05:40:57 INFO mapred.LocalJobRunner: OutputCommitter set in config null
16/03/08 05:40:57 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter
16/03/08 05:40:58 INFO mapred.LocalJobRunner: Waiting for map tasks
16/03/08 05:40:58 INFO mapred.LocalJobRunner: Starting task: attempt_local2071963236_0001_m_000000_0
16/03/08 05:40:58 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
16/03/08 05:40:58 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/ubuntu/input/wordcount.txt:0+27
16/03/08 05:40:58 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)
16/03/08 05:40:58 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100
16/03/08 05:40:58 INFO mapred.MapTask: soft limit at 83886080
16/03/08 05:40:58 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600
16/03/08 05:40:58 INFO mapred.MapTask: kvstart = 26214396; length = 6553600
16/03/08 05:40:58 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer
16/03/08 05:40:58 INFO mapred.LocalJobRunner: 
16/03/08 05:40:58 INFO mapred.MapTask: Starting flush of map output
16/03/08 05:40:58 INFO mapred.MapTask: Spilling map output
16/03/08 05:40:58 INFO mapred.MapTask: bufstart = 0; bufend = 43; bufvoid = 104857600
16/03/08 05:40:58 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214384(104857536); length = 13/6553600
16/03/08 05:40:58 INFO mapred.MapTask: Finished spill 0
16/03/08 05:40:58 INFO mapred.Task: Task:attempt_local2071963236_0001_m_000000_0 is done. And is in the process of committing
16/03/08 05:40:58 INFO mapred.LocalJobRunner: map
16/03/08 05:40:58 INFO mapred.Task: Task 'attempt_local2071963236_0001_m_000000_0' done.
16/03/08 05:40:58 INFO mapred.LocalJobRunner: Finishing task: attempt_local2071963236_0001_m_000000_0
16/03/08 05:40:58 INFO mapred.LocalJobRunner: map task executor complete.
16/03/08 05:40:58 INFO mapred.LocalJobRunner: Waiting for reduce tasks
16/03/08 05:40:58 INFO mapred.LocalJobRunner: Starting task: attempt_local2071963236_0001_r_000000_0
16/03/08 05:40:58 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]
16/03/08 05:40:58 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@12a20e8b
16/03/08 05:40:58 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10
16/03/08 05:40:58 INFO reduce.EventFetcher: attempt_local2071963236_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events
16/03/08 05:40:58 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2071963236_0001_m_000000_0 decomp: 53 len: 57 to MEMORY
16/03/08 05:40:58 INFO reduce.InMemoryMapOutput: Read 53 bytes from map-output for attempt_local2071963236_0001_m_000000_0
16/03/08 05:40:58 INFO reduce.MergeManagerImpl: closeInMemoryFile -&gt; map-output of size: 53, inMemoryMapOutputs.size() -&gt; 1, commitMemory -&gt; 0, usedMemory -&gt;53
16/03/08 05:40:58 WARN io.ReadaheadPool: Failed readahead on ifile
EBADF: Bad file descriptor
    at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)
    at org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)
    at org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)
    at org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
16/03/08 05:40:58 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning
16/03/08 05:40:58 INFO mapred.LocalJobRunner: 1 / 1 copied.
16/03/08 05:40:58 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs
16/03/08 05:40:58 INFO mapred.Merger: Merging 1 sorted segments
16/03/08 05:40:58 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 43 bytes
16/03/08 05:40:58 INFO reduce.MergeManagerImpl: Merged 1 segments, 53 bytes to disk to satisfy reduce memory limit
16/03/08 05:40:58 INFO reduce.MergeManagerImpl: Merging 1 files, 57 bytes from disk
16/03/08 05:40:58 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce
16/03/08 05:40:58 INFO mapred.Merger: Merging 1 sorted segments
16/03/08 05:40:58 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 43 bytes
16/03/08 05:40:58 INFO mapred.LocalJobRunner: 1 / 1 copied.
16/03/08 05:40:58 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords
16/03/08 05:40:58 INFO mapred.Task: Task:attempt_local2071963236_0001_r_000000_0 is done. And is in the process of committing
16/03/08 05:40:58 INFO mapred.LocalJobRunner: 1 / 1 copied.
16/03/08 05:40:58 INFO mapred.Task: Task attempt_local2071963236_0001_r_000000_0 is allowed to commit now
16/03/08 05:40:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2071963236_0001_r_000000_0' to hdfs://localhost:54310/user/ubuntu/output1/_temporary/0/task_local2071963236_0001_r_000000
16/03/08 05:40:58 INFO mapred.LocalJobRunner: reduce &gt; reduce
16/03/08 05:40:58 INFO mapred.Task: Task 'attempt_local2071963236_0001_r_000000_0' done.
16/03/08 05:40:58 INFO mapred.LocalJobRunner: Finishing task: attempt_local2071963236_0001_r_000000_0
16/03/08 05:40:58 INFO mapred.LocalJobRunner: reduce task executor complete.
16/03/08 05:40:58 INFO mapreduce.Job: Job job_local2071963236_0001 running in uber mode : false
16/03/08 05:40:58 INFO mapreduce.Job:  map 100% reduce 100%
16/03/08 05:40:58 INFO mapreduce.Job: Job job_local2071963236_0001 completed successfully
16/03/08 05:40:58 INFO mapreduce.Job: Counters: 38
    File System Counters
        FILE: Number of bytes read=6672
        FILE: Number of bytes written=518943
        FILE: Number of read operations=0
        FILE: Number of large read operations=0
        FILE: Number of write operations=0
        HDFS: Number of bytes read=54
        HDFS: Number of bytes written=35
        HDFS: Number of read operations=13
        HDFS: Number of large read operations=0
        HDFS: Number of write operations=4
    Map-Reduce Framework
        Map input records=1
        Map output records=4
        Map output bytes=43
        Map output materialized bytes=57
        Input split bytes=119
        Combine input records=4
        Combine output records=4
        Reduce input groups=4
        Reduce shuffle bytes=57
        Reduce input records=4
        Reduce output records=4
        Spilled Records=8
        Shuffled Maps =1
        Failed Shuffles=0
        Merged Map outputs=1
        GC time elapsed (ms)=72
        CPU time spent (ms)=0
        Physical memory (bytes) snapshot=0
        Virtual memory (bytes) snapshot=0
        Total committed heap usage (bytes)=241442816
    Shuffle Errors
        BAD_ID=0
        CONNECTION=0
        IO_ERROR=0
        WRONG_LENGTH=0
        WRONG_MAP=0
        WRONG_REDUCE=0
    File Input Format Counters 
        Bytes Read=27
    File Output Format Counters 
        Bytes Written=35
</code></pre>

<h3>
<a id="output-of-mapreduce-wordcount" class="anchor" href="#output-of-mapreduce-wordcount" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>Output of MapReduce WordCount</h3>

<p>Here is our output from the MR run:</p>

<pre><code>ubuntu@ip-172-31-50-38:~/hadoop/hadoopTest/WordCount$ hadoop fs -ls output1
Found 2 items
-rw-r--r--   1 ubuntu supergroup          0 2016-03-08 05:40 output1/_SUCCESS
-rw-r--r--   1 ubuntu supergroup         35 2016-03-08 05:40 output1/part-r-00000
ubuntu@ip-172-31-50-38:~/hadoop/hadoopTest/WordCount$ hadoop fs -cat output1/part-r-00000
Goodbye 1
Hadoop  2
Hello   1
</code></pre>

<h3>
<a id="github-repository-for-hadoop-folder-with-configuration" class="anchor" href="#github-repository-for-hadoop-folder-with-configuration" aria-hidden="true"><span aria-hidden="true" class="octicon octicon-link"></span></a>GitHub repository for Hadoop folder with configuration</h3>

<p>Hadoop package with configuration is available in <a href="https://github.com/ashishtam/hadoop-single-node-installation">https://github.com/ashishtam/hadoop-single-node-installation</a></p>

      <footer class="site-footer">
        <span class="site-footer-owner"><a href="https://github.com/ashishtam/hadoop-installation">Single Node Hadoop Installation on Ubuntu 14.04 Amazon EC2 Instance</a> is maintained by <a href="https://github.com/ashishtam">ashishtam</a>.</span>

        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a> using the <a href="https://github.com/jasonlong/cayman-theme">Cayman theme</a> by <a href="https://twitter.com/jasonlong">Jason Long</a>.</span>
      </footer>

    </section>

            <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
            document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
          </script>
          <script type="text/javascript">
            try {
              var pageTracker = _gat._getTracker("UA-74861100-1");
            pageTracker._trackPageview();
            } catch(err) {}
          </script>

  </body>
</html>
