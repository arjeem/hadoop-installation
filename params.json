{"name":"Single Node Hadoop Installation on Ubuntu 14.04 Amazon EC2 Instance","tagline":"by Ashish Tamrakar (UNLV)","body":"# Create the Amazon EC2 Instance (Ubuntu 14.04)\r\n\r\nSteps to set up the Amazon AWS as,\r\n### 1. Sign in to the AWS Console (Amazon Web Services)\r\n![Amazon Web Services](images/1-Amazon-Web-Services.PNG)\r\n\r\n### 2. Launch the Amazon EC2 Instance\r\n![Launch Instance](images/2-launch-instance.PNG)\r\n\t\r\n### 3. Selecting the Amazon Machine Image\r\n![Amazon Machine Image](images/3-Amazon-Machine-Image.PNG)\r\n\r\n### 4. Choose an Instance Type\r\n![Instance Type](images/4-Instance-Type.PNG)\r\n\r\n### 5. Configure Instance Details\r\n![Configuring Instance Details](images/5-Configuring-Instance-details.PNG)\r\n\r\n### 6. Configure Security Group\r\n![Configuring Security Group](images/6-Configure-Security-Group.PNG)\r\n\r\n### 7. Review and Launch Instance\r\n![Review and Launch Instance](images/7-Review-and-launch.PNG)\r\n\r\n### 8. Create key pair\r\n![Create Key Pair](images/8-Create-key-pair.PNG)\r\n\r\n### 9. Running instances\r\nNow the instance is running and you click on the Instances on the left tab to see your EC2 instances running.\r\n![Running Instances](images/9-Instances.png)\r\n\r\n## To connect your Amazon EC2 instance using SSH from your local machine (Ubuntu)\r\n\r\nIn a command line shell, change directories to the location of the private key file that you created when you launched the instance.\r\n\r\nUse the `chmod` command to make sure your private key file isn't publicly viewable. For example, if the name of your private key file is `big-data.pem`, use the following command:\r\n\r\n    $ chmod 400 big-data.pem\r\n\r\nUse the `ssh` command to connect to the instance. You'll specify the private key `(.pem)` file and `user_name@public_dns_name`. For Ubuntu, the username is `ubuntu`. \r\n\r\n    ssh -i big-data.pem ubuntu@ec2-198-51-100-1.compute-1.amazonaws.com\r\n\r\nYou'll see a response like the following:\r\n\r\n    The authenticity of host 'ec2-198-51-100-1.compute-1.amazonaws.com (10.254.142.33)' can't be established.\r\n    RSA key fingerprint is 1f:51:ae:28:bf:89:e9:d8:1f:25:5d:37:2d:7d:b8:ca:9f:f5:f1:6f.\r\n    Are you sure you want to continue connecting (yes/no)?\r\n\r\n(Optional) Verify that the fingerprint in the security alert matches the fingerprint that you obtained in step 1. If these fingerprints don't match, someone might be attempting a \"man-in-the-middle\" attack. If they match, continue to the next step.\r\nEnter `yes`.\r\nYou'll see a response like the following.\r\n\r\n    Warning: Permanently added 'ec2-198-51-100-1.compute-1.amazonaws.com' (RSA) \r\n    to the list of known hosts.\r\n\r\nIt looks something like this,\r\n\r\n![SSH to EC2 Instance from local machine](images/10-ssh-to-ec2-instance-from-local-machine.png)\r\n\r\nReference: http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AccessingInstancesLinux.html\r\n\r\n## Installation of Single Node Hadoop cluster on Ubuntu 14.04 Amazon EC2 Instance\r\n\r\n### Install Java\r\nYou need to install java by following commands as,\r\n\r\n    $ sudo add-apt-repository ppa:webupd8team/java\r\n    $ sudo apt-get update\r\n    $ sudo apt-get install oracle-java7-installer\r\n    # Update Java runtime\r\n    $ sudo update-java-alternatives -s java-7-oracle\r\n\r\nTo check whether your java is installed or not,\r\n\r\n    $ java -version\r\n\r\n### Disabling IPv6\r\nTo disable `ipv6`, you have to open `/etc/sysctl.conf` using any text editor and insert the following lines at the end:\r\n\r\n    net.ipv6.conf.all.disable_ipv6 = 1\r\n    net.ipv6.conf.default.disable_ipv6 = 1\r\n    net.ipv6.conf.lo.disable_ipv6 = 1\r\n\r\nIf `ipv6` is still not disabled, then the problem is that `sysctl.conf` is still not activated.\r\n\r\nTo solve this, open a terminal(Ctrl+Alt+T) and type the command,\r\n\r\n    $ sudo sysctl -p\r\n\r\nYou will see this in the terminal:\r\n\r\n    net.ipv6.conf.all.disable_ipv6 = 1\r\n    net.ipv6.conf.default.disable_ipv6 = 1\r\n    net.ipv6.conf.lo.disable_ipv6 = 1\r\n\r\nAfter that, if you run:\r\n\r\n    $ cat /proc/sys/net/ipv6/conf/all/disable_ipv6\r\n\r\nIt will report:\r\n\r\n    1\r\n\r\n### Install Hadoop 2.6.0 stable binaries\r\n\r\nInstall the hadoop binary and then install it on the home folder itself.\r\n\r\n    $ cd\r\n    $ wget http://mirrors.sonic.net/apache/hadoop/common/hadoop-2.6.0/hadoop-2.6.0.tar.gz\r\n    $ tar xvf hadoop-2.6.0.tar.gz\r\n    $ mv hadoop-2.6.0 hadoop\r\n\r\n\r\n### Create and Setup SSH Certificates\r\nTo create the ssh key, we create the following command as,\r\n\r\n    $ ssh-keygen -t rsa -P \"\"\r\n\r\n\r\n    Generating public/private rsa key pair.\r\n    Enter file in which to save the key (/home/ubuntu/.ssh/id_rsa): \r\n    Created directory '/home/ubuntu/.ssh'.\r\n    Your identification has been saved in /home/ubuntu/.ssh/id_rsa.\r\n    Your public key has been saved in /home/ubuntu/.ssh/id_rsa.pub.\r\n    The key fingerprint is:\r\n    50:6b:f3:fc:0f:32:bf:30:79:c2:41:71:26:cc:7d:e3 ubuntu@laptop\r\n    The key's randomart image is:\r\n    +--[ RSA 2048]----+\r\n    |        .oo.o    |\r\n    |       . .o=. o  |\r\n    |      . + .  o . |\r\n    |       o =    E  |\r\n    |        S +      |\r\n    |         . +     |\r\n    |          O +    |\r\n    |           O o   |\r\n    |            o..  |\r\n    +-----------------+\r\n\r\n\r\nYou need to copy the key to the `authorized_keys`,\r\n\r\n    $ cat $HOME/.ssh/id_rsa.pub >> $HOME/.ssh/authorized_keys\r\n\r\n## Setting Up Hadoop Environment\r\n\r\n### Configuring .bashrc file\r\nYou need to add the following lines on `.bashrc` file under the folder `/home/ubuntu` as,\r\n\r\n    $ vim /home/ubuntu/.bashrc\r\n\r\nAdd the following lines:\r\n\r\n    #HADOOP VARIABLES START\r\n    export JAVA_HOME=/usr/lib/jvm/java-7-oracle\r\n    export HADOOP_INSTALL=/home/ubuntu/hadoop\r\n    export PATH=$PATH:$HADOOP_INSTALL/bin\r\n    export PATH=$PATH:$HADOOP_INSTALL/sbin\r\n    export HADOOP_MAPRED_HOME=$HADOOP_INSTALL\r\n    export HADOOP_COMMON_HOME=$HADOOP_INSTALL\r\n    export HADOOP_HDFS_HOME=$HADOOP_INSTALL\r\n    export YARN_HOME=$HADOOP_INSTALL\r\n    export HADOOP_COMMON_LIB_NATIVE_DIR=$HADOOP_INSTALL/lib/native\r\n    export HADOOP_OPTS=\"-Djava.library.path=$HADOOP_INSTALL/lib/native\"\r\n    export HADOOP_CLASSPATH=${JAVA_HOME}/lib/tools.jar\r\n    #HADOOP VARIABLES END\r\n\r\n### Update hadoop-env.sh\r\nUpdate the `JAVA_HOME` in `/home/ubuntu/hadoop/etc/hadoop/hadoop-env.sh` as,\r\n\r\n    $ vim /home/ubuntu/hadoop/etc/hadoop/hadoop-env.sh\r\n\r\nFind the `export JAVA_HOME` line and replace `{JAVA_HOME}` with the actual path as,\r\n\r\n    #Find this line and update with the path\r\n    export JAVA_HOME=/usr/lib/jvm/java-7-oracle\r\n\r\n## Configuring hadoop configuration files\r\n\r\n### Add/Update core-site.xml\r\nAdd/Update the `core-site.xml` file by running the following command as,\r\n\r\n    $ vim /home/ubuntu/hadoop/etc/hadoop/core-site.xml\r\n\r\nAdd the following code under the `<configuration></configuration>` as,\r\n\r\n    <configuration>\r\n    <property>\r\n      <name>hadoop.tmp.dir</name>\r\n      <value>/home/ubuntu/hadoop/tmp</value>\r\n      <description>Temporary Directory.</description>\r\n    </property>\r\n    \r\n    <property>\r\n      <name>fs.default.name</name>\r\n      <value>hdfs://localhost:54310</value>\r\n      <description>Use HDFS as file storage engine</description>\r\n    </property>\r\n    </configuration>\r\n\r\nAlso, you need to create the `tmp` folder on `/home/ubuntu/hadoop` as,\r\n\r\n    $ mkdir /home/ubuntu/hadoop/tmp\r\n\r\n### Add/update mapred-site.xml\r\nIf you find the `mapred-site.xml` file inside `/home/ubuntu/hadoop/etc/hadoop/` then you just need to update inside the `mapred-site.xml` file as shown below. \r\nOtherwise, you need to copy the `mapred-site.xml.template` to create `mapred-site.xml` file as,\r\n\r\nCommands to create the file (Skip this process if there is already `mapred-site.xml` file inside `/home/ubuntu/hadoop/etc/hadoop` folder),\r\n\r\n    $ cd /home/ubuntu/hadoop/etc/hadoop\r\n    $ cp mapred-site.xml.template mapred-site.xml\r\n    $ vim mapred-site.xml\r\n\r\nNow, update `mapred-site.xml` file under the `<configuration></configuration>` as,\r\n\r\n    <configuration>\r\n    \t<property>\r\n\t\t<name>mapred.job.tracker</name>\r\n\t\t<value>localhost:54311</value>\r\n\t\t<description>The host and port that the MapReduce job tracker runs\r\n\t\tat.  If \"local\", then jobs are run in-process as a single map\r\n\t\tand reduce task.\r\n\t\t</description>\r\n\t </property>\r\n    </configuration>\r\n\r\n### Add/update hdfs-site.xml\r\nUpdating the `hdfs-site.xml` file as,\r\n\r\n    $ vim /home/ubuntu/hadoop/etc/hadoop/hdfs-site.xml\r\n\r\nEdit the `hdfs-site.xml` file under the `<configuration></configuration>` as,\r\n\r\n    <configuration>\r\n      <property>\r\n        <name>dfs.replication</name>\r\n        <value>1</value>\r\n        <description>Default block replication.\r\n        The actual number of replications can be specified when the file is created.\r\n        The default is used if replication is not specified in create time.\r\n        </description>\r\n      </property>\r\n      <property>\r\n         <name>dfs.namenode.name.dir</name>\r\n         <value>file:///home/ubuntu/hadoop/hdfs/namenode</value>\r\n      </property>\r\n      <property>\r\n         <name>dfs.datanode.data.dir</name>\r\n         <value>file:///home/ubuntu/hadoop/hdfs/datanode</value>\r\n      </property>\r\n    </configuration>\r\n\r\nNow you need to create two folders: `namenode` and `datanode` under the folders `/home/ubuntu/hadoop/hdfs` as,\r\n\r\n    $ mkdir -p /home/ubuntu/hadoop/hdfs/namenode\r\n    $ mkdir -p /home/ubuntu/hadoop/hdfs/datanode\r\n\r\n## Format the New Hadoop File System\r\n\r\nNow, the Hadoop file system needs to be formatted so that we can start to use it. The format command should be issued with write permission since it creates current directory\r\n\r\n    $ hadoop namenode -format\r\n\r\n## Starting Hadoop\r\nTo start the hadoop, we use `start-all.sh` or (`start-dfs.sh` and `start-yarn.sh` individually) as\r\n\r\n    $ start-all.sh\r\n\r\nOR\r\n\r\n    $ start-dfs.sh\r\n    $ start-yarn.sh\r\n\r\nThe command will result in output as,\r\n\r\n    ubuntu@ip-172-31-50-38:~$ start-all.sh\r\n    This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh\r\n    Starting namenodes on [localhost]\r\n    localhost: starting namenode, logging to /home/ubuntu/hadoop/logs/hadoop-ubuntu-namenode-ip-172-31-50-38.out\r\n    localhost: starting datanode, logging to /home/ubuntu/hadoop/logs/hadoop-ubuntu-datanode-ip-172-31-50-38.out\r\n    Starting secondary namenodes [0.0.0.0]\r\n    0.0.0.0: starting secondarynamenode, logging to /home/ubuntu/hadoop/logs/hadoop-ubuntu-secondarynamenode-ip-172-31-50-38.out\r\n    starting yarn daemons\r\n    starting resourcemanager, logging to /home/ubuntu/hadoop/logs/yarn-ubuntu-resourcemanager-ip-172-31-50-38.out\r\n    localhost: starting nodemanager, logging to /home/ubuntu/hadoop/logs/yarn-ubuntu-nodemanager-ip-172-31-50-38.out\r\n\r\nWe can check if it's really up and running:\r\n\r\n    $ jps\r\n    5180 SecondaryNameNode\r\n    3567 ResourceManager\r\n    5412 NodeManager\r\n    5442 Jps\r\n    4836 NameNode\r\n    4993 DataNode\r\n\r\n## Testing MapReduce Job\r\nFor testing MapReduce job, you can test it with the jar files inside share/hadoop/mapreduce folder as,\r\n\r\n\t~/hadoop$ hadoop jar ./share/hadoop/mapreduce/hadoop-mapreduce-examples-2.4.1.jar pi 2 5\r\n\tNumber of Maps  = 2\r\n\tSamples per Map = 5\r\n\t14/07/14 01:28:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n\tWrote input for Map #0\r\n\tWrote input for Map #1\r\n\tStarting Job\r\n\t14/07/14 01:28:07 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\r\n\t14/07/14 01:28:07 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\r\n\t14/07/14 01:28:07 INFO input.FileInputFormat: Total input paths to process : 2\r\n\t14/07/14 01:28:07 INFO mapreduce.JobSubmitter: number of splits:2\r\n\t14/07/14 01:28:09 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local1228885165_0001\r\n\t...\r\n\t\tFile Input Format Counters \r\n\t\t\tBytes Read=236\r\n\t\tFile Output Format Counters \r\n\t\t\tBytes Written=97\r\n\tJob Finished in 6.072 seconds\r\n\tEstimated value of Pi is 3.60000000000000000000\r\n\r\n## MapReduce example with WordCount\r\n\r\nWe have a file `wordcount.txt` which has the following content as,\r\n\r\n\t$ cat wordcount.txt\r\n\tHello Hadoop, Goodbye Hadoop.\r\n\r\nLet's look at Hadoop file system:\r\n\r\n\t$ hadoop fs -ls\r\n\r\nNow we want to copy the input file to hdfs:\r\n\r\n\t$ hadoop fs -mkdir input\r\n\t$ hadoop fs -put wordcount.txt input/\r\n\t$ hadoop fs -ls input\r\n\t$ hadoop fs -cat input/wordcount.txt\r\n\tHello Hadoop, Goodbye Hadoop.\r\n\r\n### Creating Java file for WordCount\r\n\r\n    import java.io.IOException;\r\n    import java.util.StringTokenizer;\r\n    \r\n    import org.apache.hadoop.conf.Configuration;\r\n    import org.apache.hadoop.fs.Path;\r\n    import org.apache.hadoop.io.IntWritable;\r\n    import org.apache.hadoop.io.Text;\r\n    import org.apache.hadoop.mapreduce.Job;\r\n    import org.apache.hadoop.mapreduce.Mapper;\r\n    import org.apache.hadoop.mapreduce.Reducer;\r\n    import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;\r\n    import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;\r\n    \r\n    public class WordCount {\r\n    \r\n      public static class TokenizerMapper\r\n           extends Mapper<Object, Text, Text, IntWritable>{\r\n    \r\n        private final static IntWritable one = new IntWritable(1);\r\n        private Text word = new Text();\r\n    \r\n        public void map(Object key, Text value, Context context\r\n                        ) throws IOException, InterruptedException {\r\n          StringTokenizer itr = new StringTokenizer(value.toString());\r\n          while (itr.hasMoreTokens()) {\r\n            word.set(itr.nextToken());\r\n            context.write(word, one);\r\n          }\r\n        }\r\n      }\r\n    \r\n      public static class IntSumReducer\r\n           extends Reducer<Text,IntWritable,Text,IntWritable> {\r\n        private IntWritable result = new IntWritable();\r\n    \r\n        public void reduce(Text key, Iterable<IntWritable> values,\r\n                           Context context\r\n                           ) throws IOException, InterruptedException {\r\n          int sum = 0;\r\n          for (IntWritable val : values) {\r\n            sum += val.get();\r\n          }\r\n          result.set(sum);\r\n          context.write(key, result);\r\n        }\r\n      }\r\n    \r\n      public static void main(String[] args) throws Exception {\r\n        Configuration conf = new Configuration();\r\n        Job job = Job.getInstance(conf, \"word count\");\r\n        job.setJarByClass(WordCount.class);\r\n        job.setMapperClass(TokenizerMapper.class);\r\n        job.setCombinerClass(IntSumReducer.class);\r\n        job.setReducerClass(IntSumReducer.class);\r\n        job.setOutputKeyClass(Text.class);\r\n        job.setOutputValueClass(IntWritable.class);\r\n        FileInputFormat.addInputPath(job, new Path(args[0]));\r\n        FileOutputFormat.setOutputPath(job, new Path(args[1]));\r\n        System.exit(job.waitForCompletion(true) ? 0 : 1);\r\n      }\r\n    }\r\n\r\n\r\n### Making jar file from the java code\r\n\r\n    $ hadoop com.sun.tools.javac.Main WordCount.java \r\n    $ jar cf wordcount.jar WordCount*.class\r\n\r\n\r\n### Running wordcount MapReduce\r\n\r\n\tubuntu@ip-172-31-50-38:~/hadoop/hadoopTest/WordCount$ hadoop jar wordcount.jar WordCount input/wordcount.txt output1\r\n\t16/03/08 05:40:56 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\r\n\t16/03/08 05:40:56 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\r\n\t16/03/08 05:40:57 WARN mapreduce.JobSubmitter: Hadoop command-line option parsing not performed. Implement the Tool interface and execute your application with ToolRunner to remedy this.\r\n\t16/03/08 05:40:57 INFO input.FileInputFormat: Total input paths to process : 1\r\n\t16/03/08 05:40:57 INFO mapreduce.JobSubmitter: number of splits:1\r\n\t16/03/08 05:40:57 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2071963236_0001\r\n\t16/03/08 05:40:57 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\r\n\t16/03/08 05:40:57 INFO mapreduce.Job: Running job: job_local2071963236_0001\r\n\t16/03/08 05:40:57 INFO mapred.LocalJobRunner: OutputCommitter set in config null\r\n\t16/03/08 05:40:57 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter\r\n\t16/03/08 05:40:58 INFO mapred.LocalJobRunner: Waiting for map tasks\r\n\t16/03/08 05:40:58 INFO mapred.LocalJobRunner: Starting task: attempt_local2071963236_0001_m_000000_0\r\n\t16/03/08 05:40:58 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\r\n\t16/03/08 05:40:58 INFO mapred.MapTask: Processing split: hdfs://localhost:54310/user/ubuntu/input/wordcount.txt:0+27\r\n\t16/03/08 05:40:58 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\r\n\t16/03/08 05:40:58 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\r\n\t16/03/08 05:40:58 INFO mapred.MapTask: soft limit at 83886080\r\n\t16/03/08 05:40:58 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\r\n\t16/03/08 05:40:58 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\r\n\t16/03/08 05:40:58 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\r\n\t16/03/08 05:40:58 INFO mapred.LocalJobRunner: \r\n\t16/03/08 05:40:58 INFO mapred.MapTask: Starting flush of map output\r\n\t16/03/08 05:40:58 INFO mapred.MapTask: Spilling map output\r\n\t16/03/08 05:40:58 INFO mapred.MapTask: bufstart = 0; bufend = 43; bufvoid = 104857600\r\n\t16/03/08 05:40:58 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214384(104857536); length = 13/6553600\r\n\t16/03/08 05:40:58 INFO mapred.MapTask: Finished spill 0\r\n\t16/03/08 05:40:58 INFO mapred.Task: Task:attempt_local2071963236_0001_m_000000_0 is done. And is in the process of committing\r\n\t16/03/08 05:40:58 INFO mapred.LocalJobRunner: map\r\n\t16/03/08 05:40:58 INFO mapred.Task: Task 'attempt_local2071963236_0001_m_000000_0' done.\r\n\t16/03/08 05:40:58 INFO mapred.LocalJobRunner: Finishing task: attempt_local2071963236_0001_m_000000_0\r\n\t16/03/08 05:40:58 INFO mapred.LocalJobRunner: map task executor complete.\r\n\t16/03/08 05:40:58 INFO mapred.LocalJobRunner: Waiting for reduce tasks\r\n\t16/03/08 05:40:58 INFO mapred.LocalJobRunner: Starting task: attempt_local2071963236_0001_r_000000_0\r\n\t16/03/08 05:40:58 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\r\n\t16/03/08 05:40:58 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@12a20e8b\r\n\t16/03/08 05:40:58 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=363285696, maxSingleShuffleLimit=90821424, mergeThreshold=239768576, ioSortFactor=10, memToMemMergeOutputsThreshold=10\r\n\t16/03/08 05:40:58 INFO reduce.EventFetcher: attempt_local2071963236_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\r\n\t16/03/08 05:40:58 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local2071963236_0001_m_000000_0 decomp: 53 len: 57 to MEMORY\r\n\t16/03/08 05:40:58 INFO reduce.InMemoryMapOutput: Read 53 bytes from map-output for attempt_local2071963236_0001_m_000000_0\r\n\t16/03/08 05:40:58 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 53, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->53\r\n\t16/03/08 05:40:58 WARN io.ReadaheadPool: Failed readahead on ifile\r\n\tEBADF: Bad file descriptor\r\n\t\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posix_fadvise(Native Method)\r\n\t\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.posixFadviseIfPossible(NativeIO.java:267)\r\n\t\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator.posixFadviseIfPossible(NativeIO.java:146)\r\n\t\tat org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl.run(ReadaheadPool.java:206)\r\n\t\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)\r\n\t\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)\r\n\t\tat java.lang.Thread.run(Thread.java:745)\r\n\t16/03/08 05:40:58 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\r\n\t16/03/08 05:40:58 INFO mapred.LocalJobRunner: 1 / 1 copied.\r\n\t16/03/08 05:40:58 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\r\n\t16/03/08 05:40:58 INFO mapred.Merger: Merging 1 sorted segments\r\n\t16/03/08 05:40:58 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 43 bytes\r\n\t16/03/08 05:40:58 INFO reduce.MergeManagerImpl: Merged 1 segments, 53 bytes to disk to satisfy reduce memory limit\r\n\t16/03/08 05:40:58 INFO reduce.MergeManagerImpl: Merging 1 files, 57 bytes from disk\r\n\t16/03/08 05:40:58 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\r\n\t16/03/08 05:40:58 INFO mapred.Merger: Merging 1 sorted segments\r\n\t16/03/08 05:40:58 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 43 bytes\r\n\t16/03/08 05:40:58 INFO mapred.LocalJobRunner: 1 / 1 copied.\r\n\t16/03/08 05:40:58 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\r\n\t16/03/08 05:40:58 INFO mapred.Task: Task:attempt_local2071963236_0001_r_000000_0 is done. And is in the process of committing\r\n\t16/03/08 05:40:58 INFO mapred.LocalJobRunner: 1 / 1 copied.\r\n\t16/03/08 05:40:58 INFO mapred.Task: Task attempt_local2071963236_0001_r_000000_0 is allowed to commit now\r\n\t16/03/08 05:40:58 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2071963236_0001_r_000000_0' to hdfs://localhost:54310/user/ubuntu/output1/_temporary/0/task_local2071963236_0001_r_000000\r\n\t16/03/08 05:40:58 INFO mapred.LocalJobRunner: reduce > reduce\r\n\t16/03/08 05:40:58 INFO mapred.Task: Task 'attempt_local2071963236_0001_r_000000_0' done.\r\n\t16/03/08 05:40:58 INFO mapred.LocalJobRunner: Finishing task: attempt_local2071963236_0001_r_000000_0\r\n\t16/03/08 05:40:58 INFO mapred.LocalJobRunner: reduce task executor complete.\r\n\t16/03/08 05:40:58 INFO mapreduce.Job: Job job_local2071963236_0001 running in uber mode : false\r\n\t16/03/08 05:40:58 INFO mapreduce.Job:  map 100% reduce 100%\r\n\t16/03/08 05:40:58 INFO mapreduce.Job: Job job_local2071963236_0001 completed successfully\r\n\t16/03/08 05:40:58 INFO mapreduce.Job: Counters: 38\r\n\t\tFile System Counters\r\n\t\t\tFILE: Number of bytes read=6672\r\n\t\t\tFILE: Number of bytes written=518943\r\n\t\t\tFILE: Number of read operations=0\r\n\t\t\tFILE: Number of large read operations=0\r\n\t\t\tFILE: Number of write operations=0\r\n\t\t\tHDFS: Number of bytes read=54\r\n\t\t\tHDFS: Number of bytes written=35\r\n\t\t\tHDFS: Number of read operations=13\r\n\t\t\tHDFS: Number of large read operations=0\r\n\t\t\tHDFS: Number of write operations=4\r\n\t\tMap-Reduce Framework\r\n\t\t\tMap input records=1\r\n\t\t\tMap output records=4\r\n\t\t\tMap output bytes=43\r\n\t\t\tMap output materialized bytes=57\r\n\t\t\tInput split bytes=119\r\n\t\t\tCombine input records=4\r\n\t\t\tCombine output records=4\r\n\t\t\tReduce input groups=4\r\n\t\t\tReduce shuffle bytes=57\r\n\t\t\tReduce input records=4\r\n\t\t\tReduce output records=4\r\n\t\t\tSpilled Records=8\r\n\t\t\tShuffled Maps =1\r\n\t\t\tFailed Shuffles=0\r\n\t\t\tMerged Map outputs=1\r\n\t\t\tGC time elapsed (ms)=72\r\n\t\t\tCPU time spent (ms)=0\r\n\t\t\tPhysical memory (bytes) snapshot=0\r\n\t\t\tVirtual memory (bytes) snapshot=0\r\n\t\t\tTotal committed heap usage (bytes)=241442816\r\n\t\tShuffle Errors\r\n\t\t\tBAD_ID=0\r\n\t\t\tCONNECTION=0\r\n\t\t\tIO_ERROR=0\r\n\t\t\tWRONG_LENGTH=0\r\n\t\t\tWRONG_MAP=0\r\n\t\t\tWRONG_REDUCE=0\r\n\t\tFile Input Format Counters \r\n\t\t\tBytes Read=27\r\n\t\tFile Output Format Counters \r\n\t\t\tBytes Written=35\r\n\r\n### Output of MapReduce WordCount\r\nHere is our output from the MR run:\r\n\r\n\tubuntu@ip-172-31-50-38:~/hadoop/hadoopTest/WordCount$ hadoop fs -ls output1\r\n\tFound 2 items\r\n\t-rw-r--r--   1 ubuntu supergroup          0 2016-03-08 05:40 output1/_SUCCESS\r\n\t-rw-r--r--   1 ubuntu supergroup         35 2016-03-08 05:40 output1/part-r-00000\r\n\tubuntu@ip-172-31-50-38:~/hadoop/hadoopTest/WordCount$ hadoop fs -cat output1/part-r-00000\r\n\tGoodbye\t1\r\n\tHadoo\t1\r\n\tHadoop\t1\r\n\tHello\t1\r\n\r\n\r\n### GitHub repository for Hadoop folder with configuration\r\nHadoop package with configuration is available in https://github.com/ashishtam/hadoop-single-node-installation","google":"UA-74861100-1","note":"Don't delete this file! It's used internally to help with page regeneration."}